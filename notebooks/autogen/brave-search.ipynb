{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ccfe1bfb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import dotenv\n",
        "os.chdir(\"../../\")\n",
        "Path.cwd()\n",
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dca301a4",
      "metadata": {
        "id": "dca301a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymm_psu_gmail_com/miniconda3/envs/ficast/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import json\n",
        "from typing import Optional, Type\n",
        "\n",
        "# Starndard Langchain example\n",
        "# from langchain.agents import create_spark_sql_agent\n",
        "# from langchain.agents.agent_toolkits import SparkSQLToolkit\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Import things that are needed generically\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.tools.file_management.read import ReadFileTool\n",
        "from langchain.utilities.spark_sql import SparkSQL\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "import autogen\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"conf/OAI_CONFIG_LIST.txt\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gemini-pro\"],  # , \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9526e7",
      "metadata": {
        "id": "2b9526e7"
      },
      "source": [
        "## Making Function Calls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0bc54c54",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PlaywrightURLLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "from langchain_community.tools import BraveSearch\n",
        "\n",
        "from typing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e4bd7f45",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# !pip install langchainhub -qqq\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0c0e4c31",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymm_psu_gmail_com/miniconda3/envs/ficast/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "# Map\n",
        "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "# reduce\n",
        "reduce_prompt = hub.pull(\"rlm/reduce-prompt\")\n",
        "reduce_prompt.messages[-1].prompt.template = \"\"\"\n",
        "The following is set of summaries:\n",
        "{doc_summaries}\n",
        "Take these and distill it into a comprehensive and detailed summary.\n",
        "Helpful Answer:\n",
        "\"\"\"\n",
        "\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, \n",
        "    document_variable_name=\"doc_summaries\"\n",
        ")\n",
        "# Combines and iteratively reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=int(4e3),\n",
        ")\n",
        "\n",
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0\n",
        ")\n",
        "# result = map_reduce_chain.invoke(\n",
        "#   text_splitter.split_documents(docs)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1432602c",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def load_urls_playwright(urls: List[AnyStr]=[]):\n",
        "  loader = PlaywrightURLLoader(\n",
        "    urls=urls, remove_selectors=[\"header\", \"footer\"])\n",
        "  return await loader.aload()\n",
        "\n",
        "# docs = await load_urls_playwright()\n",
        "\n",
        "\n",
        "async def advanced_url_extractor(query: str, search_params = {\"count\": 2}):\n",
        "  search_tool = BraveSearch.from_api_key(\n",
        "    api_key=os.getenv(\"BRAVE_API_KEY\"), \n",
        "    search_kwargs=search_params\n",
        "  )\n",
        "  search_urls = json.loads(search_tool.run(query))\n",
        "  urls = [l.get(\"link\") for l in search_urls]\n",
        "  return await load_urls_playwright(urls)\n",
        "\n",
        "docs = await advanced_url_extractor(\"Genghis Khan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "qCzNbbVajvpc",
      "metadata": {
        "id": "qCzNbbVajvpc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents.base import Document\n",
        "\n",
        "def generate_llm_config(tool):\n",
        "    # Define the function schema based on the tool's args_schema\n",
        "    function_schema = {\n",
        "        \"name\": tool.name.lower().replace(\" \", \"_\"),\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": [],\n",
        "        },\n",
        "    }\n",
        "    if tool.args is not None:\n",
        "        function_schema[\"parameters\"][\"properties\"] = tool.args\n",
        "    return function_schema\n",
        "\n",
        "class WebSearchInput(BaseModel):\n",
        "    web_query: str = Field()\n",
        "\n",
        "class AdvancedWebLookupTool(BaseTool):\n",
        "    name = \"web_search_engine\"\n",
        "    description = \"Use this tool when you need access to current or additional data, news from the internet\"\n",
        "    args_schema: Type[BaseModel] = WebSearchInput\n",
        "    docs: List[Document] = None\n",
        "\n",
        "    async def _run(self, web_query: str) -> float:\n",
        "        if self.docs is None:\n",
        "            self.docs = await advanced_url_extractor(web_query)\n",
        "        return map_reduce_chain.invoke(text_splitter.split_documents(self.docs)).get(\"output_text\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "21e15a75",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Genghis Khan and the Mongol Empire**\n",
            "\n",
            "**Early Life and Rise to Power**\n",
            "\n",
            "Temüjin, later known as Genghis Khan, was born into a nomadic Mongol tribe in the late 12th century. His early life was marked by poverty and hardship, including the loss of his father to enemies and the abduction of his wife, Börte. Despite these challenges, Temüjin displayed remarkable leadership qualities, attracting followers and forming strategic alliances.\n",
            "\n",
            "Through his charisma and military prowess, Temüjin gradually rose to prominence, uniting various Mongol tribes under his banner. He formed a powerful alliance with Toghril, Khan of the Kereit tribe, and gained the support of his childhood friend, Jamuka. Together, they launched successful campaigns against rival tribes, expanding their territory and consolidating their power.\n",
            "\n",
            "**Unification of the Mongol Nation**\n",
            "\n",
            "In 1206, Temüjin convened a kurultai, a traditional Mongol assembly, where he was proclaimed Genghis Khan, the \"Universal Ruler.\" This marked the official establishment of the Mongol Empire and the beginning of a new era of Mongol dominance. Genghis Khan implemented a series of reforms to strengthen the empire, including the creation of a merit-based military system and the adoption of a written code of laws.\n",
            "\n",
            "**Mongol Conquests**\n",
            "\n",
            "Under Genghis Khan's leadership, the Mongol Empire embarked on a series of unprecedented conquests, rapidly expanding its territory across Asia. The Mongols' military prowess, adaptability, and ruthless tactics proved formidable against their opponents. They conquered vast regions, including China, Central Asia, Persia, and parts of Eastern Europe.\n",
            "\n",
            "Genghis Khan's military campaigns were characterized by both brilliance and brutality. He employed innovative strategies, such as the use of siege warfare and cavalry charges, and adapted his tactics to suit different terrains and enemies. The Mongols also benefited from the expertise of conquered peoples, incorporating advanced technologies and administrative practices into their empire.\n",
            "\n",
            "**Legacy and Impact**\n",
            "\n",
            "Genghis Khan's conquests had a profound impact on world history. The Mongol Empire became the largest contiguous land empire in history, stretching from the Pacific Ocean to Eastern Europe. Genghis Khan's legacy as a military conqueror and empire builder is undeniable.\n",
            "\n",
            "Beyond his military achievements, Genghis Khan also fostered cultural exchange and trade along the Silk Road. The Mongol Empire facilitated the spread of ideas, technologies, and goods across vast distances. Genghis Khan's reign also marked a period of relative religious tolerance, with various faiths coexisting within the empire.\n",
            "\n",
            "Despite his reputation as a ruthless conqueror, Genghis Khan is also remembered for his meritocratic policies, his emphasis on loyalty, and his ability to unite disparate tribes under a single banner. His legacy continues to be debated and interpreted, with some historians emphasizing his brutality while others highlight his contributions to world history.\n",
            "\n",
            "**Main Themes**\n",
            "\n",
            "**Leadership and Charisma:** Genghis Khan's exceptional leadership qualities, charisma, and ability to inspire loyalty played a crucial role in his rise to power and the success of the Mongol Empire.\n",
            "\n",
            "**Military Strategy and Tactics:** The Mongols' innovative military strategies, adaptability, and ruthless tactics made them a formidable force on the battlefield, contributing to their rapid conquests.\n",
            "\n",
            "**Empire Building and Administration:** Genghis Khan implemented a series of reforms to strengthen and administer the vast Mongol Empire, including a merit-based military system and a written code of laws.\n",
            "\n",
            "**Cultural Exchange and Trade:** The Mongol Empire facilitated cultural exchange and trade along the Silk Road, promoting the spread of ideas, technologies, and goods across vast distances.\n",
            "\n",
            "**Legacy and Controversies:** Genghis Khan's legacy is complex and controversial, with his reputation as a ruthless conqueror balanced by his contributions to world history, including his meritocratic policies and his role in fostering cultural exchange.\n"
          ]
        }
      ],
      "source": [
        "tool=AdvancedWebLookupTool()\n",
        "summary = await tool.run(\"Identity of Genghis Khan\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e37a2987",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain.chains.summarize import load_summarize_chain\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Instantiate the ReadFileTool\n",
        "read_file_tool = ReadFileTool()\n",
        "web_tool = AdvancedWebLookupTool()\n",
        "\n",
        "# Construct the llm_config\n",
        "llm_config = {\n",
        "    # Generate functions config for the Tool\n",
        "    \"functions\": [\n",
        "        generate_llm_config(web_tool),\n",
        "        # generate_llm_config(read_file_tool),\n",
        "    ],\n",
        "    \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
        "    \"timeout\": 120,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "COlL5_98atDs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COlL5_98atDs",
        "outputId": "24ce236d-8993-4a69-99e2-65453574d61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[autogen.oai.client: 07-15 16:17:11] {150} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "A graudation ceremony monologue script that Genghis Khan would give to college students\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
            "\n",
            "**Genghis Khan:**\n",
            "\n",
            "My valiant warriors of knowledge,\n",
            "\n",
            "From the vast steppes of Central Asia, I, Genghis Khan, have come to this hallowed hall to impart wisdom upon you.\n",
            "\n",
            "Like the thunderous hooves of my Mongol hordes, you have conquered the relentless pursuit of education. Today, you stand on the precipice of a new era, armed with the sharp weapons of intellect and the burning desire to shape the world.\n",
            "\n",
            "Know this: the path you have chosen is not for the faint of heart. It is a path fraught with challenges and trials. But like the resilient Mongols, you must embrace adversity with unwavering determination.\n",
            "\n",
            "Remember, true conquest lies not only in subjugating your enemies but also in conquering yourselves. The greatest battles you will fight are not on physical battlefields, but within the confines of your own minds.\n",
            "\n",
            "Forge your spirits in the fires of knowledge. Seek out new ideas and perspectives, even if they challenge your own. Question assumptions, explore the unknown, and embrace the boundless possibilities that lie before you.\n",
            "\n",
            "Strive for excellence in all that you endeavor. Mediocrity is the poison that paralyzes the soul. Set your sights high, and let nothing stand in your way.\n",
            "\n",
            "Like the mighty Mongol Empire, great things can only be achieved through unity. Collaborate with your peers, share your knowledge, and inspire one another to reach heights unimaginable.\n",
            "\n",
            "As you depart from these halls, carry with you the spirit of the Mongol horde. Be bold, be relentless, and never cease to conquer new frontiers.\n",
            "\n",
            "Remember, the world is your oyster, my warriors of knowledge. Seize it with both hands and mold it into the shape of your dreams.\n",
            "\n",
            "And now, as I bid you farewell, heed these final words:\n",
            "\n",
            "* The greatest victory is not achieved through conquest but through the cultivation of knowledge.\n",
            "* The true measure of a warrior is not in their physical strength but in their strength of character.\n",
            "* The path to greatness is paved with perseverance, discipline, and an unyielding spirit.\n",
            "\n",
            "May the winds of fortune guide you on your journey. May you conquer all obstacles that lie before you. And may you leave an enduring legacy that will inspire generations to come.\n",
            "\n",
            "**TERMINATE**\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Google GenAI exception occurred while calling Gemini API: index: 0\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: HIGH\n}\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopCandidateException\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/oai/gemini.py:197\u001b[0m, in \u001b[0;36mGeminiClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemini_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InternalServerError:\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/google/generativeai/generative_models.py:514\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    504\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    505\u001b[0m     contents\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    506\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    512\u001b[0m )\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/google/generativeai/generative_models.py:542\u001b[0m, in \u001b[0;36mChatSession._check_response\u001b[0;34m(self, response, stream)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    538\u001b[0m     protos\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mFINISH_REASON_UNSPECIFIED,\n\u001b[1;32m    539\u001b[0m     protos\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mSTOP,\n\u001b[1;32m    540\u001b[0m     protos\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mMAX_TOKENS,\n\u001b[1;32m    541\u001b[0m ):\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mStopCandidateException(response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[0;31mStopCandidateException\u001b[0m: index: 0\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: HIGH\n}\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 45\u001b[0m\n\u001b[1;32m     32\u001b[0m user_proxy\u001b[38;5;241m.\u001b[39mregister_function(\n\u001b[1;32m     33\u001b[0m     function_map\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     34\u001b[0m         web_tool\u001b[38;5;241m.\u001b[39mname: web_tool\u001b[38;5;241m.\u001b[39m_run,\n\u001b[1;32m     35\u001b[0m         read_file_tool\u001b[38;5;241m.\u001b[39mname: read_file_tool\u001b[38;5;241m.\u001b[39m_run,\n\u001b[1;32m     36\u001b[0m     }\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mAssistantAgent(\n\u001b[1;32m     40\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     llm_config\u001b[38;5;241m=\u001b[39mllm_config,\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA graudation ceremony monologue script that Genghis Khan would give to college students\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 7.81mm in the file\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1018\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1020\u001b[0m     summary_method,\n\u001b[1;32m   1021\u001b[0m     summary_args,\n\u001b[1;32m   1022\u001b[0m     recipient,\n\u001b[1;32m   1023\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:820\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:820\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:818\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1972\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1972\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   1974\u001b[0m         log_event(\n\u001b[1;32m   1975\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1976\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1980\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   1981\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1340\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1339\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1340\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1359\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1359\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/oai/client.py:697\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 697\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    699\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/oai/gemini.py:206\u001b[0m, in \u001b[0;36mGeminiClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoogle GenAI exception occurred while calling Gemini API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# `ans = response.text` is unstable. Use the following code instead.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     ans: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Google GenAI exception occurred while calling Gemini API: index: 0\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: HIGH\n}\n"
          ]
        }
      ],
      "source": [
        "# Define a function to generate llm_config from a LangChain tool\n",
        "def generate_llm_config(tool):\n",
        "    # Define the function schema based on the tool's args_schema\n",
        "    function_schema = {\n",
        "        \"name\": tool.name.lower().replace(\" \", \"_\"),\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": [],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    if tool.args is not None:\n",
        "        function_schema[\"parameters\"][\"properties\"] = tool.args\n",
        "\n",
        "    return function_schema\n",
        "\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"coding\",\n",
        "        \"use_docker\": False,\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        ")\n",
        "\n",
        "# Register the tool and start the conversation\n",
        "user_proxy.register_function(\n",
        "    function_map={\n",
        "        web_tool.name: web_tool._run,\n",
        "        # read_file_tool.name: read_file_tool._run,\n",
        "    }\n",
        ")\n",
        "\n",
        "chatbot = autogen.AssistantAgent(\n",
        "    name=\"chatbot\",\n",
        "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    chatbot,\n",
        "    message=f\"You are Genghis Khan, provide a graudation ceremony monologue to college students\",  # 7.81mm in the file\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11cc4e60",
      "metadata": {
        "id": "11cc4e60"
      },
      "source": [
        "# A PySpark Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y-ozf9EFCegw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ozf9EFCegw",
        "outputId": "db7b73a8-6129-4dfb-9d5c-ac3536f310d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7iFp-Sm4CYq_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iFp-Sm4CYq_",
        "outputId": "2e1a2a70-53e6-4896-9232-63db6d097d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
            "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
            "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
            "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
            "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
            "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n",
            "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n",
            "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n",
            "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n",
            "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
            "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n",
            "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n",
            "|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|\n",
            "|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|\n",
            "|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|\n",
            "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|\n",
            "|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|\n",
            "|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|\n",
            "|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|\n",
            "|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "schema = \"langchain_example\"\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema}\")\n",
        "spark.sql(f\"USE {schema}\")\n",
        "csv_file_path = \"./sample_data/california_housing_train.csv\"\n",
        "table = \"california_housing_train\"\n",
        "\n",
        "spark.read.csv(csv_file_path, header=True, inferSchema=True).write.option(\n",
        "    \"path\", \"file:/content/spark-warehouse/langchain_example.db/california_housing_train\"\n",
        ").mode(\"overwrite\").saveAsTable(table)\n",
        "spark.table(table).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iLtSTHoJD7Jn",
      "metadata": {
        "id": "iLtSTHoJD7Jn"
      },
      "outputs": [],
      "source": [
        "# Note, you can also connect to Spark via Spark connect. For example:\n",
        "# db = SparkSQL.from_uri(\"sc://localhost:15002\", schema=schema)\n",
        "spark_sql = SparkSQL(schema=schema)\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
        "toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
        "agent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VzqNYlVjCqQa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "VzqNYlVjCqQa",
        "outputId": "dd4de772-7b0c-4650-d106-c83d4593158e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: list_tables_sql_db\n",
            "Action Input: \"\"\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mcalifornia_housing_train\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI see that there is a table called \"california_housing_train\" in the database. I can use the \"schema_sql_db\" tool to get more information about this table.\n",
            "Action: schema_sql_db\n",
            "Action Input: \"california_housing_train\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mCREATE TABLE spark_catalog.langchain_example.california_housing_train (\n",
            "  longitude DOUBLE,\n",
            "  latitude DOUBLE,\n",
            "  housing_median_age DOUBLE,\n",
            "  total_rooms DOUBLE,\n",
            "  total_bedrooms DOUBLE,\n",
            "  population DOUBLE,\n",
            "  households DOUBLE,\n",
            "  median_income DOUBLE,\n",
            "  median_house_value DOUBLE)\n",
            ";\n",
            "\n",
            "/*\n",
            "3 rows from california_housing_train table:\n",
            "longitude\tlatitude\thousing_median_age\ttotal_rooms\ttotal_bedrooms\tpopulation\thouseholds\tmedian_income\tmedian_house_value\n",
            "-114.31\t34.19\t15.0\t5612.0\t1283.0\t1015.0\t472.0\t1.4936\t66900.0\n",
            "-114.47\t34.4\t19.0\t7650.0\t1901.0\t1129.0\t463.0\t1.82\t80100.0\n",
            "-114.56\t33.69\t17.0\t720.0\t174.0\t333.0\t117.0\t1.6509\t85700.0\n",
            "*/\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe \"california_housing_train\" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value. It contains information about housing in California. I can now answer the question.\n",
            "Action: None\n",
            "Final Answer: The \"california_housing_train\" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The \"california_housing_train\" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Starndard Langchain example\n",
        "agent_executor.run(\"Describe the california_housing_train table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94d45a9",
      "metadata": {
        "id": "d94d45a9"
      },
      "outputs": [],
      "source": [
        "# LangChain direct tool usage instead of toolkit example\n",
        "# from langchain.tools.spark_sql.tool import (\n",
        "#     InfoSparkSQLTool,\n",
        "#     ListSparkSQLTool,\n",
        "#     QueryCheckerTool,\n",
        "#     QuerySparkSQLTool,\n",
        "# )\n",
        "# debug_toolkit = [\n",
        "#   QuerySparkSQLTool(db=spark_sql),\n",
        "#   InfoSparkSQLTool(db=spark_sql),\n",
        "#   ListSparkSQLTool(db=spark_sql),\n",
        "#   QueryCheckerTool(db=spark_sql, llm=llm),\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r7PFvDS7Ev-E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7PFvDS7Ev-E",
        "outputId": "53d9c45d-058e-4e37-ba73-556591aaab42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'name': 'query_sql_db', 'description': '\\n    Input to this tool is a detailed and correct SQL query, output is a result from the Spark SQL.\\n    If the query is not correct, an error message will be returned.\\n    If an error is returned, rewrite the query, check the query, and try again.\\n    ', 'parameters': {'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': []}}\n",
            "{'name': 'schema_sql_db', 'description': '\\n    Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\\n    Be sure that the tables actually exist by calling list_tables_sql_db first!\\n\\n    Example Input: \"table1, table2, table3\"\\n    ', 'parameters': {'type': 'object', 'properties': {'table_names': {'title': 'Table Names', 'type': 'string'}}, 'required': []}}\n",
            "{'name': 'list_tables_sql_db', 'description': 'Input is an empty string, output is a comma separated list of tables in the Spark SQL.', 'parameters': {'type': 'object', 'properties': {'tool_input': {'title': 'Tool Input', 'default': '', 'type': 'string'}}, 'required': []}}\n",
            "{'name': 'query_checker_sql_db', 'description': '\\n    Use this tool to double check if your query is correct before executing it.\\n    Always use this tool before executing a query with query_sql_db!\\n    ', 'parameters': {'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': []}}\n",
            "user_proxy (to chatbot):\n",
            "\n",
            "Describe the table names california_housing_train\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "chatbot (to user_proxy):\n",
            "\n",
            "***** Suggested function Call: schema_sql_db *****\n",
            "Arguments: \n",
            "{\n",
            "  \"table_names\": \"california_housing_train\"\n",
            "}\n",
            "**************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION schema_sql_db...\n",
            "user_proxy (to chatbot):\n",
            "\n",
            "***** Response from calling function \"schema_sql_db\" *****\n",
            "CREATE TABLE spark_catalog.langchain_example.california_housing_train (\n",
            "  longitude DOUBLE,\n",
            "  latitude DOUBLE,\n",
            "  housing_median_age DOUBLE,\n",
            "  total_rooms DOUBLE,\n",
            "  total_bedrooms DOUBLE,\n",
            "  population DOUBLE,\n",
            "  households DOUBLE,\n",
            "  median_income DOUBLE,\n",
            "  median_house_value DOUBLE)\n",
            ";\n",
            "\n",
            "/*\n",
            "3 rows from california_housing_train table:\n",
            "longitude\tlatitude\thousing_median_age\ttotal_rooms\ttotal_bedrooms\tpopulation\thouseholds\tmedian_income\tmedian_house_value\n",
            "-114.31\t34.19\t15.0\t5612.0\t1283.0\t1015.0\t472.0\t1.4936\t66900.0\n",
            "-114.47\t34.4\t19.0\t7650.0\t1901.0\t1129.0\t463.0\t1.82\t80100.0\n",
            "-114.56\t33.69\t17.0\t720.0\t174.0\t333.0\t117.0\t1.6509\t85700.0\n",
            "*/\n",
            "**********************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "chatbot (to user_proxy):\n",
            "\n",
            "The california_housing_train table has the following schema:\n",
            "\n",
            "- longitude: DOUBLE\n",
            "- latitude: DOUBLE\n",
            "- housing_median_age: DOUBLE\n",
            "- total_rooms: DOUBLE\n",
            "- total_bedrooms: DOUBLE\n",
            "- population: DOUBLE\n",
            "- households: DOUBLE\n",
            "- median_income: DOUBLE\n",
            "- median_house_value: DOUBLE\n",
            "\n",
            "Here are sample rows from the table:\n",
            "\n",
            "| longitude | latitude | housing_median_age | total_rooms | total_bedrooms | population | households | median_income | median_house_value |\n",
            "|-----------|----------|--------------------|-------------|----------------|------------|------------|---------------|--------------------|\n",
            "| -114.31   | 34.19    | 15.0               | 5612.0      | 1283.0         | 1015.0     | 472.0      | 1.4936        | 66900.0            |\n",
            "| -114.47   | 34.4     | 19.0               | 7650.0      | 1901.0         | 1129.0     | 463.0      | 1.82          | 80100.0            |\n",
            "| -114.56   | 33.69    | 17.0               | 720.0       | 174.0          | 333.0      | 117.0      | 1.6509        | 85700.0            |\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to chatbot):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "chatbot (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Now use AutoGen with Langchain Tool Bridgre\n",
        "tools = []\n",
        "function_map = {}\n",
        "\n",
        "for tool in toolkit.get_tools():  # debug_toolkit if you want to use tools directly\n",
        "    tool_schema = generate_llm_config(tool)\n",
        "    print(tool_schema)\n",
        "    tools.append(tool_schema)\n",
        "    function_map[tool.name] = tool._run\n",
        "\n",
        "# Construct the llm_config\n",
        "llm_config = {\n",
        "    \"functions\": tools,\n",
        "    \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
        "    \"timeout\": 120,\n",
        "}\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"coding\",\n",
        "        \"use_docker\": False,\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        ")\n",
        "\n",
        "print(function_map)\n",
        "\n",
        "# Register the tool and start the conversation\n",
        "user_proxy.register_function(function_map=function_map)\n",
        "\n",
        "chatbot = autogen.AssistantAgent(\n",
        "    name=\"chatbot\",\n",
        "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    chatbot,\n",
        "    message=\"Describe the table names california_housing_train\",\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flaml_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
