{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowing Human Feedback in Agents\n",
    "\n",
    "Idea:\n",
    "1. Human input will be requested but it is silent by default (no input)\n",
    "2. Human may interject (raise a signal) to participate and insert comment in the up coming chat turn around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, dotenv, autogen\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "import autogen\n",
    "from autogen.io.websockets import IOWebsockets, IOStream\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gemini,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "llm_config_stream = llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from autogen import ConversableAgent\n",
    "from autogen.agentchat.conversable_agent import colored, content_str, Dict, Union, Agent, OpenAIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyConversableAgent(ConversableAgent):   \n",
    "    pass     \n",
    "    # def _print_received_message(self, message: Union[Dict, str], sender: Agent):\n",
    "    #     iostream = IOStream.get_default()\n",
    "    #     # print the message received\n",
    "    #     iostream.print(colored(sender.name, \"yellow\"), \"(to\", f\"{self.name}):\\n\", flush=True)\n",
    "    #     message = self._message_to_dict(message)\n",
    "    #     print(message)\n",
    "    #     if message.get(\"tool_responses\"):  # Handle tool multi-call responses\n",
    "    #         for tool_response in message[\"tool_responses\"]:\n",
    "    #             self._print_received_message(tool_response, sender)\n",
    "    #         if message.get(\"role\") == \"tool\":\n",
    "    #             return  # If role is tool, then content is just a concatenation of all tool_responses\n",
    "\n",
    "    #     if message.get(\"role\") in [\"function\", \"tool\"]:\n",
    "    #         if message[\"role\"] == \"function\":\n",
    "    #             id_key = \"name\"\n",
    "    #         else:\n",
    "    #             id_key = \"tool_call_id\"\n",
    "    #         id = message.get(id_key, \"No id found\")\n",
    "    #         func_print = f\"***** Response from calling {message['role']} ({id}) *****\"\n",
    "    #         iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "    #         iostream.print(message[\"content\"], flush=True)\n",
    "    #         iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "    #     else:\n",
    "    #         content = message.get(\"content\")\n",
    "    #         if content is not None:\n",
    "    #             if \"context\" in message:\n",
    "    #                 content = OpenAIWrapper.instantiate(\n",
    "    #                     content,\n",
    "    #                     message[\"context\"],\n",
    "    #                     self.llm_config and self.llm_config.get(\"allow_format_str_template\", False),\n",
    "    #                 )\n",
    "    #             iostream.print(content_str(content), flush=True)\n",
    "    #         if \"function_call\" in message and message[\"function_call\"]:\n",
    "    #             function_call = dict(message[\"function_call\"])\n",
    "    #             func_print = (\n",
    "    #                 f\"***** Suggested function call: {function_call.get('name', '(No function name found)')} *****\"\n",
    "    #             )\n",
    "    #             iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "    #             iostream.print(\n",
    "    #                 \"Arguments: \\n\",\n",
    "    #                 function_call.get(\"arguments\", \"(No arguments found)\"),\n",
    "    #                 flush=True,\n",
    "    #                 sep=\"\",\n",
    "    #             )\n",
    "    #             iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "    #         if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
    "    #             for tool_call in message[\"tool_calls\"]:\n",
    "    #                 id = tool_call.get(\"id\", \"No tool call id found\")\n",
    "    #                 function_call = dict(tool_call.get(\"function\", {}))\n",
    "    #                 func_print = f\"***** Suggested tool call ({id}): {function_call.get('name', '(No function name found)')} *****\"\n",
    "    #                 iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "    #                 iostream.print(\n",
    "    #                     \"Arguments: \\n\",\n",
    "    #                     function_call.get(\"arguments\", \"(No arguments found)\"),\n",
    "    #                     flush=True,\n",
    "    #                     sep=\"\",\n",
    "    #                 )\n",
    "    #                 iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "\n",
    "    #     iostream.print(\"\\n\", \"-\" * 80, flush=True, sep=\"\")\n",
    "    #     iostream.print(json.dumps({\"message\": message, \"sender\": sender.name}))\n",
    "\n",
    "class HandRaiseConversableAgent(ConversableAgent):\n",
    "    def __init__(self, *args, websocket_uri=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.websocket_uri = websocket_uri  # WebSocket for monitoring\n",
    "        self.hand_raise = False  # Initial state\n",
    "        self.default_message = \"skip\"  # Default message\n",
    "\n",
    "    async def monitor_hand_raise(self):\n",
    "        \"\"\"Asynchronously monitor the WebSocket for hand-raise signals.\"\"\"\n",
    "        if not self.websocket_uri:\n",
    "            return\n",
    "\n",
    "        async with IOWebsockets.connect(self.websocket_uri) as ws:\n",
    "            async for message in ws:\n",
    "                if message == \"raise-hand\":\n",
    "                    print(\"Hand-raise detected!\")\n",
    "                    self.hand_raise = True\n",
    "                elif message == \"lower-hand\":\n",
    "                    print(\"Hand-raise cleared.\")\n",
    "                    self.hand_raise = False\n",
    "\n",
    "    def get_human_input(self, prompt: str) -> str:\n",
    "        \"\"\"Override to handle human input based on hand-raise signal.\"\"\"\n",
    "        if self.hand_raise:\n",
    "            print(\"Hand-raise detected. Awaiting human input...\")\n",
    "            iostream = IOStream.get_default()\n",
    "            reply = iostream.input(prompt)  # Non-blocking input from IOStream\n",
    "            self._human_input.append(reply)\n",
    "            return reply\n",
    "        else:\n",
    "            print(f\"No hand-raise detected. Returning default message: '{self.default_message}'\")\n",
    "            return self.default_message\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = ConversableAgent(\n",
    "    \"agent1\",\n",
    "    system_message=\"You are have an indefinite conversation for fun, you can talk about anything but you must always reply and add a question for following up.\",\n",
    "    llm_config=llm_config_stream,\n",
    "    # is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"NEVER\",  # never ask for human input\n",
    ")\n",
    "agent2 = ConversableAgent(\n",
    "    \"agent2\",\n",
    "    system_message=\"You are have an indefinite conversation for fun, you can talk about anything but you must always reply and add a question for following up.\",\n",
    "    llm_config=llm_config_stream,\n",
    "    # is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"NEVER\",  # never ask for human input\n",
    ")\n",
    "\n",
    "human_proxy = HandRaiseConversableAgent(\n",
    "    \"human_proxy\",\n",
    "    llm_config=False,  # no LLM used for human proxy\n",
    "    is_termination_msg=lambda msg: \"CORRECT!\" in msg[\"content\"],\n",
    "    # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
    ")\n",
    "def custom_speaker_selection_func(last_speaker: Agent, groupchat: autogen.GroupChat):\n",
    "    \"\"\"Define a customized speaker selection function.\n",
    "    A recommended way is to define a transition for each speaker in the groupchat.\n",
    "\n",
    "    Returns:\n",
    "        Return an `Agent` class or a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages\n",
    "    if len(messages) <= 1:\n",
    "        return agent1\n",
    "    if len(messages) <= 2:\n",
    "        return agent2\n",
    "    \n",
    "    if last_speaker is human_proxy:\n",
    "        if messages[-2][\"name\"] == \"agent1\":\n",
    "            # If it is the planning stage, let the planner to continue\n",
    "            return agent2\n",
    "        elif messages[-2][\"name\"] == \"agent2\":\n",
    "            # If the last message is from the scientist, let the scientist to continue\n",
    "            return agent1\n",
    "\n",
    "    elif last_speaker in [agent1, agent2]:\n",
    "        # Always let the user to speak after the agent\n",
    "        return human_proxy\n",
    "    \n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[human_proxy, agent1, agent2],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=custom_speaker_selection_func,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect(iostream: IOWebsockets) -> None:\n",
    "    print(f\" - on_connect(): Connected to client using IOWebsockets {iostream}\", flush=True)\n",
    "\n",
    "    try:\n",
    "        # 1. Receive Initial Message\n",
    "        initial_msg = \"talk about anything you want\"\n",
    "        # initial_msg = iostream.input()  # Blocking until a message is received\n",
    "        # if initial_msg:\n",
    "        #     print(f\"Received message from client: {initial_msg}\", flush=True)\n",
    "\n",
    "        with IOStream.set_default(iostream):\n",
    "            # 2. Initiate the chat with the agent\n",
    "            print(f\"Initiating chat with agent using message '{initial_msg}'\", flush=True)\n",
    "            # This is where your chat initiation logic happens\n",
    "            agent1.initiate_chat(\n",
    "                manager, \n",
    "                message=initial_msg,\n",
    "                clear_history=False  # Set clear_history based on your business logic\n",
    "            )\n",
    "        \n",
    "        # 3. After the chat initiation, close the connection\n",
    "        # print(\"Closing WebSocket connection after chat initiation.\", flush=True)\n",
    "        # IOStream.get_default().close()  # Close the IOStream connection\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions and ensure the connection is closed in case of failure\n",
    "        print(f\"Error during WebSocket communication: {str(e)}\", flush=True)\n",
    "        IOStream.get_default().close()  # Close the IOStream connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during WebSocket communication: received 1005 (no status received [internal]); then sent 1005 (no status received [internal])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " - IOWebsockets._handler(): Error in on_connect: 'IOWebsockets' object has no attribute 'close'\n"
     ]
    }
   ],
   "source": [
    "from contextlib import asynccontextmanager  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "from fastapi import FastAPI  # noqa: E402\n",
    "from fastapi.responses import HTMLResponse  # noqa: E402\n",
    "\n",
    "html_path = \"agentchat_websocket_server/chat.html\"\n",
    "\n",
    "@asynccontextmanager\n",
    "async def run_websocket_server(app):\n",
    "    try:\n",
    "        with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8080) as uri:\n",
    "            print(f\"WebSocket server started at {uri}.\", flush=True)\n",
    "            yield\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket server failed: {str(e)}\", flush=True)\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=run_websocket_server)\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def get():\n",
    "    html_file = Path(html_path)\n",
    "    html_content = html_file.read_text()\n",
    "    return HTMLResponse(content=html_content, media_type=\"text/html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [328227]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket server started at ws://127.0.0.1:8080.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:35922 - \"GET / HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_328227/2332427647.py\", line 25, in get\n",
      "    html_content = html_file.read_text()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/pathlib.py\", line 1027, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/pathlib.py\", line 1013, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../agentchat_websocket_server/chat.html'\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [328227]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn  # noqa: E402\n",
    "\n",
    "config = uvicorn.Config(app)\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()  # noqa: F704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code above, you will be prompt to enter a response\n",
    "each time it is your turn to speak. You can see the human in the conversation\n",
    "was not very good at guessing the number... but hey the agent was nice enough\n",
    "to give out the number in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Input Mode = `TERMINATE`\n",
    "\n",
    "In this mode, human input is only requested when a termination condition is\n",
    "met. **If the human chooses to intercept and reply, the counter will be reset**; if \n",
    "the human chooses to skip, the automatic reply mechanism will be used; if the human\n",
    "chooses to terminate, the conversation will be terminated.\n",
    "\n",
    "Let us see this mode in action by playing the same game again, but this time\n",
    "the guessing agent will only have two chances to guess the number, and if it \n",
    "fails, the human will be asked to provide feedback,\n",
    "and the guessing agent gets two more chances.\n",
    "If the correct number is guessed eventually, the conversation will be terminated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
