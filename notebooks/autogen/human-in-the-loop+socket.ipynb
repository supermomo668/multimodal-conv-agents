{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowing Human Feedback in Agents\n",
    "\n",
    "Idea:\n",
    "1. Human input will be requested but it is silent by default (no input)\n",
    "2. Human may interject (raise a signal) to participate and insert comment in the up coming chat turn around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, dotenv, autogen\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "import autogen\n",
    "from autogen.io.websockets import IOWebsockets, IOStream\n",
    "\n",
    "\n",
    "os.chdir(\"../../\")\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"conf/OAI_CONFIG_LIST.txt\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gemini,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "llm_config_stream = llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from autogen import ConversableAgent\n",
    "from autogen.agentchat.conversable_agent import colored, content_str, Dict, Union, Agent, OpenAIWrapper\n",
    "\n",
    "class MyConversableAgent(ConversableAgent):        \n",
    "    def _print_received_message(self, message: Union[Dict, str], sender: Agent):\n",
    "        iostream = IOStream.get_default()\n",
    "        # print the message received\n",
    "        iostream.print(colored(sender.name, \"yellow\"), \"(to\", f\"{self.name}):\\n\", flush=True)\n",
    "        message = self._message_to_dict(message)\n",
    "\n",
    "        if message.get(\"tool_responses\"):  # Handle tool multi-call responses\n",
    "            for tool_response in message[\"tool_responses\"]:\n",
    "                self._print_received_message(tool_response, sender)\n",
    "            if message.get(\"role\") == \"tool\":\n",
    "                return  # If role is tool, then content is just a concatenation of all tool_responses\n",
    "\n",
    "        if message.get(\"role\") in [\"function\", \"tool\"]:\n",
    "            if message[\"role\"] == \"function\":\n",
    "                id_key = \"name\"\n",
    "            else:\n",
    "                id_key = \"tool_call_id\"\n",
    "            id = message.get(id_key, \"No id found\")\n",
    "            func_print = f\"***** Response from calling {message['role']} ({id}) *****\"\n",
    "            iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "            iostream.print(message[\"content\"], flush=True)\n",
    "            iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "        else:\n",
    "            content = message.get(\"content\")\n",
    "            if content is not None:\n",
    "                if \"context\" in message:\n",
    "                    content = OpenAIWrapper.instantiate(\n",
    "                        content,\n",
    "                        message[\"context\"],\n",
    "                        self.llm_config and self.llm_config.get(\"allow_format_str_template\", False),\n",
    "                    )\n",
    "                iostream.print(content_str(content), flush=True)\n",
    "            if \"function_call\" in message and message[\"function_call\"]:\n",
    "                function_call = dict(message[\"function_call\"])\n",
    "                func_print = (\n",
    "                    f\"***** Suggested function call: {function_call.get('name', '(No function name found)')} *****\"\n",
    "                )\n",
    "                iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "                iostream.print(\n",
    "                    \"Arguments: \\n\",\n",
    "                    function_call.get(\"arguments\", \"(No arguments found)\"),\n",
    "                    flush=True,\n",
    "                    sep=\"\",\n",
    "                )\n",
    "                iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "            if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    id = tool_call.get(\"id\", \"No tool call id found\")\n",
    "                    function_call = dict(tool_call.get(\"function\", {}))\n",
    "                    func_print = f\"***** Suggested tool call ({id}): {function_call.get('name', '(No function name found)')} *****\"\n",
    "                    iostream.print(colored(func_print, \"green\"), flush=True)\n",
    "                    iostream.print(\n",
    "                        \"Arguments: \\n\",\n",
    "                        function_call.get(\"arguments\", \"(No arguments found)\"),\n",
    "                        flush=True,\n",
    "                        sep=\"\",\n",
    "                    )\n",
    "                    iostream.print(colored(\"*\" * len(func_print), \"green\"), flush=True)\n",
    "\n",
    "        iostream.print(\"\\n\", \"-\" * 80, flush=True, sep=\"\")\n",
    "        iostream.print(json.dumps({\"message\": message, \"sender\": sender.name}))\n",
    "        \n",
    "agent_with_number = MyConversableAgent(\n",
    "    \"agent_with_number\",\n",
    "    system_message=\"You are have an indefinite conversation for fun, you can talk about anything but you must always reply and add a question for following up.\",\n",
    "    llm_config=llm_config_stream,\n",
    "    # is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"NEVER\",  # never ask for human input\n",
    ")\n",
    "\n",
    "human_proxy = MyConversableAgent(\n",
    "    \"human_proxy\",\n",
    "    llm_config=False,  # no LLM used for human proxy\n",
    "    is_termination_msg=lambda msg: \"CORRECT!\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect(iostream: IOWebsockets) -> None:\n",
    "    print(f\" - on_connect(): Connected to client using IOWebsockets {iostream}\", flush=True)\n",
    "\n",
    "    try:\n",
    "        # 1. Receive Initial Message\n",
    "        initial_msg = iostream.input()  # Blocking until a message is received\n",
    "        if initial_msg:\n",
    "            print(f\"Received message from client: {initial_msg}\", flush=True)\n",
    "\n",
    "        with IOStream.set_default(iostream):\n",
    "            # 2. Initiate the chat with the agent\n",
    "            print(f\"Initiating chat with agent using message '{initial_msg}'\", flush=True)\n",
    "            # This is where your chat initiation logic happens\n",
    "            human_proxy.initiate_chat(\n",
    "                agent_with_number, \n",
    "                message=initial_msg,\n",
    "                clear_history=False  # Set clear_history based on your business logic\n",
    "            )\n",
    "        \n",
    "        # 3. After the chat initiation, close the connection\n",
    "        print(\"Closing WebSocket connection after chat initiation.\", flush=True)\n",
    "        IOStream.get_default().close()  # Close the IOStream connection\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions and ensure the connection is closed in case of failure\n",
    "        print(f\"Error during WebSocket communication: {str(e)}\", flush=True)\n",
    "        IOStream.get_default().close()  # Close the IOStream connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "from fastapi import FastAPI  # noqa: E402\n",
    "from fastapi.responses import HTMLResponse  # noqa: E402\n",
    "\n",
    "html_path = \"notebooks/autogen/agentchat_websocket_server/chat.html\"\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def run_websocket_server(app):\n",
    "    try:\n",
    "        with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8080) as uri:\n",
    "            print(f\"WebSocket server started at {uri}.\", flush=True)\n",
    "            yield\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket server failed: {str(e)}\", flush=True)\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=run_websocket_server)\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def get():\n",
    "    html_file = Path(html_path)\n",
    "    html_content = html_file.read_text()\n",
    "    return HTMLResponse(content=html_content, media_type=\"text/html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3163160]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket server started at ws://127.0.0.1:8080.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59984 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59984 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:59978 - \"GET / HTTP/1.1\" 200 OK\n",
      " - on_connect(): Connected to client using IOWebsockets <autogen.io.websockets.IOWebsockets object at 0x7f050967ffe0>\n",
      "Received message from client: hi\n",
      "Initiating chat with agent using message 'hi'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [3163160]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn  # noqa: E402\n",
    "\n",
    "config = uvicorn.Config(app)\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()  # noqa: F704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code above, you will be prompt to enter a response\n",
    "each time it is your turn to speak. You can see the human in the conversation\n",
    "was not very good at guessing the number... but hey the agent was nice enough\n",
    "to give out the number in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Input Mode = `TERMINATE`\n",
    "\n",
    "In this mode, human input is only requested when a termination condition is\n",
    "met. **If the human chooses to intercept and reply, the counter will be reset**; if \n",
    "the human chooses to skip, the automatic reply mechanism will be used; if the human\n",
    "chooses to terminate, the conversation will be terminated.\n",
    "\n",
    "Let us see this mode in action by playing the same game again, but this time\n",
    "the guessing agent will only have two chances to guess the number, and if it \n",
    "fails, the human will be asked to provide feedback,\n",
    "and the guessing agent gets two more chances.\n",
    "If the correct number is guessed eventually, the conversation will be terminated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
