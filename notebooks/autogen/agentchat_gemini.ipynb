{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cee7e3",
   "metadata": {},
   "source": [
    "# Using Gemini in AutoGen with Other LLMs\n",
    "\n",
    "You don't need to handle OpenAI or Google's GenAI packages. AutoGen already handled all of these for you.\n",
    "\n",
    "You can just create different agents with different backend LLM with assistant agent, and all models/agents are at your fingertip.\n",
    "\n",
    "\n",
    "## Main Distinctions\n",
    "- Gemini does not have the \"system_message\" field (correct me if I am wrong). So, it's instruction following skills are not as strong as GPTs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801d13b",
   "metadata": {},
   "source": [
    "Sample OAI_CONFIG_LIST \n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-35-turbo\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"base_url\": \"https://tnrllmproxy.azurewebsites.net/v1\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"dalle\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-pro\",\n",
    "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
    "        \"api_type\": \"google\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-pro-vision\",\n",
    "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
    "        \"api_type\": \"google\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fee066",
   "metadata": {},
   "source": [
    "### Before everything starts, install AutoGen with the `gemini` option\n",
    "```bash\n",
    "pip install \"pyautogen[gemini]~=0.2.0b4\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Install These Missing Packages Manually if You Encounter Any Errors\n",
    "```bash\n",
    "pip install https://github.com/microsoft/autogen/archive/gemini.zip\n",
    "pip install \"google-generativeai\" \"pydash\" \"pillow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37cd165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mymm_psu_gmail_com/miniconda3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent\n",
    "\n",
    "from autogen.agentchat.contrib.img_utils import get_image_data, _to_pil\n",
    "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
    "\n",
    "from termcolor import colored\n",
    "import random\n",
    "\n",
    "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb2d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed6b642",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'OAI_CONFIG_LIST'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config_list_4v \u001b[38;5;241m=\u001b[39m \u001b[43mautogen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_list_from_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOAI_CONFIG_LIST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-vision-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m config_list_gpt4 \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_json(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAI_CONFIG_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     filter_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-0314\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-32k\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-32k-0314\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-32k-v0314\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m     },\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m config_list_gemini \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_json(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAI_CONFIG_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     filter_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     },\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/autogen/oai/openai_utils.py:520\u001b[0m, in \u001b[0;36mconfig_list_from_json\u001b[0;34m(env_or_file, file_location, filter_dict)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m         config_list_path \u001b[38;5;241m=\u001b[39m env_or_file\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_list_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m    521\u001b[0m         config_list \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filter_config(config_list, filter_dict)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OAI_CONFIG_LIST'"
     ]
    }
   ],
   "source": [
    "\n",
    "config_list_4v = autogen.config_list_from_json(\n",
    "    \"conf/OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-vision-preview\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list_gemini_vision = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro-vision\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3221e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd4b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60857e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f09481",
   "metadata": {},
   "source": [
    "## Gemini Assitant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c6e552",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Sort the array with Bubble Sort: [4, 1, 3, 2]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "```python\n",
      "# filename: bubble_sort.py\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    arr = [4, 1, 3, 2]\n",
      "    print(\"Unsorted array: \", arr)\n",
      "    sorted_arr = bubble_sort(arr)\n",
      "    print(\"Sorted array: \", sorted_arr)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Unsorted array:  [4, 1, 3, 2]\n",
      "Sorted array:  [1, 2, 3, 4]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant = AssistantAgent(\"assistant\", \n",
    "                           llm_config={\"config_list\": config_list_gemini, \"seed\": 42}, \n",
    "                           max_consecutive_auto_reply=3)\n",
    "# print(assistant.system_message)\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", \n",
    "                            code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}, \n",
    "                            human_input_mode=\"NEVER\", \n",
    "                           is_termination_msg = lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0)\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=\"Sort the array with Bubble Sort: [4, 1, 3, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de85984",
   "metadata": {},
   "source": [
    "## Agent Collaboration and Interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2f2bb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "Do Transformers buy auto insurance or health insurance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "Transformers, being fictional robots, do not require auto or health insurance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "If Transformers were real and required insurance, would their coverage be a specialized form of property insurance instead of typical auto or health insurance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "Yes, if Transformers were real and required insurance, their coverage would likely be a specialized form of property insurance designed specifically for sentient robotic beings.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "Considering their dual nature as vehicles and sentient beings, could they potentially require a unique insurance policy that covers both aspects, merging elements of auto, health, and property insurance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "Yes, the unique nature of Transformers as both vehicles and sentient beings could necessitate a hybrid insurance policy that combines elements of auto, health, and property insurance to comprehensively cover their dual aspects.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "In a scenario where Transformers are integrated into human society, how would liability be determined in an accident involving a Transformer in vehicle form—is the Transformer itself at fault, or would it be considered a driver error?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "Liability in an accident involving a Transformer in vehicle form would depend on the specific circumstances, but could potentially be attributed to the Transformer itself if it was deemed to be acting autonomously, or to a human driver if they were controlling the Transformer at the time of the accident.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gpt = AssistantAgent(\"GPT-4\", \n",
    "                     system_message=\"\"\"You should ask weird, tricky, and concise questions. \n",
    "Ask the next question based on (by evolving) the previous one.\"\"\",\n",
    "                       llm_config={\"config_list\": config_list_gpt4, \"seed\": 42}, \n",
    "                       max_consecutive_auto_reply=3)\n",
    "\n",
    "gemini = AssistantAgent(\"Gemini-Pro\", \n",
    "                     system_message=\"\"\"Always answer questions within one sentence. \"\"\",\n",
    "#                      system_message=\"answer:\",\n",
    "                       llm_config={\"config_list\": config_list_gemini, \"seed\": 42}, \n",
    "                       max_consecutive_auto_reply=4)\n",
    "\n",
    "\n",
    "\n",
    "gpt.initiate_chat(gemini, message=\"Do Transformers buy auto insurance or health insurance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c079aa7",
   "metadata": {},
   "source": [
    "Let's switch position. Now, Gemini is the question raiser. \n",
    "\n",
    "This time, Gemini could not follow the system instruction well or evolve questions, because the Gemini does not handle system messages similar to GPTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1ead90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "Should Spider-Man invest in 401K?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "As a fictional character, Spider-Man does not require a 401(k), but his alter ego, if employed with such benefits, might consider it for retirement savings.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "If the sun sneezes, does it become stars?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "No, a sneeze is a biological function that the sun, being a star, cannot perform.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "If you could have any animal as your neighbor, who would it be and why a giraffe?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGPT-4\u001b[0m (to Gemini-Pro):\n",
      "\n",
      "A giraffe could be an interesting neighbor due to its unique height, allowing it to assist with tasks such as fetching items from tall trees or serving as a lookout.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mGemini-Pro\u001b[0m (to GPT-4):\n",
      "\n",
      "If a cow decided to wear pants, what would be in its pockets?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gpt = AssistantAgent(\"GPT-4\", \n",
    "                     system_message=\"\"\"Always answer questions within one sentence. \"\"\",\n",
    "                       llm_config={\"config_list\": config_list_gpt4, \"seed\": 42}, \n",
    "                       max_consecutive_auto_reply=3)\n",
    "\n",
    "gemini = AssistantAgent(\"Gemini-Pro\", \n",
    "                     system_message=\"\"\"You should ask weird, tricky, and concise questions. \n",
    "Ask the next question based on (by evolving) the previous one.\"\"\",\n",
    "                       llm_config={\"config_list\": config_list_gemini, \"seed\": 42}, \n",
    "                       max_consecutive_auto_reply=4)\n",
    "\n",
    "gemini.initiate_chat(gpt, message=\"Should Spider-Man invest in 401K?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf9c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35635e3",
   "metadata": {},
   "source": [
    "## Gemini RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f66022",
   "metadata": {},
   "source": [
    "Here we will be exploring RAG with Gemini. Note that Gemini will raise a 500 error if a message is an empty string. To prevent this, we set the `default_auto_reply` to `Reply plaintext TERMINATE to exit.` for the `ragproxyagent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ea1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list_gemini,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    default_auto_reply=\"Reply plaintext TERMINATE to exit.\",  # Gemini will raise 500 error if the response is empty.\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            os.path.join(os.path.abspath(''), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list_gemini[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False, # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=code_problem, search_string=\"spark\")  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09139b42",
   "metadata": {},
   "source": [
    "## Gemini Multimodal\n",
    "\n",
    "You can create multimodal agent for Gemini the same way as the GPT-4V and LLaVA.\n",
    "\n",
    "\n",
    "Note that the Gemini-pro-vision does not support chat yet. So, we only use the last message in the prompt for multi-turn chat. The behavior might be strange compared to GPT-4V and LLaVA models.\n",
    "\n",
    "Here, we ask a question about \n",
    "![](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5098e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_agent = MultimodalConversableAgent(\"Gemini Vision\", \n",
    "                           llm_config={\"config_list\": config_list_gemini_vision, \"seed\": 42}, \n",
    "                           max_consecutive_auto_reply=1)\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", \n",
    "                            human_input_mode=\"NEVER\",\n",
    "                            max_consecutive_auto_reply=0)\n",
    "\n",
    "# user_proxy.initiate_chat(image_agent, \n",
    "#                          message=\"\"\"What's the breed of this dog? \n",
    "# <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\"\"\")\n",
    "\n",
    "user_proxy.initiate_chat(image_agent, \n",
    "                         message=\"\"\"What's this image about? \n",
    "<img https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png?raw=true>.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c3d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fc7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d7ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b56419",
   "metadata": {},
   "source": [
    "## GPT  v.s. Gemini Arena\n",
    "\n",
    "Do you remember AutoGen can ask LLMs to play chess. Here, we create an arena to allow GPT and Gemini fight together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18ca19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47c9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
