{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mymm_psu_gmail_com/hackathon/rag-agents/thought-agents')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import autogen\n",
    "# from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent\n",
    "\n",
    "# from autogen.agentchat.contrib.img_utils import get_image_data, _to_pil\n",
    "# from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
    "\n",
    "# from termcolor import colored\n",
    "import random\n",
    "\n",
    "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang\n",
    "\n",
    "#\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "os.chdir(\"../../\")\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_754641/2698333353.py:10: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../../conf/dialogue\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_config=AutogenLLMConfig(config_list=[{'model': 'gemini-1.5-pro', 'api_key': 'AIzaSyBxWRGKZT3ZcOW7LIo_a0q7sG8vd-OFl-w', 'api_type': 'google', 'safety_settings': [{'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}]}], model='gemini-1.5-pro', filter_dict={'model': ['gemini-1.5-pro']}, config_list_path='conf/OAI_CONFIG_LIST.txt') podcast_config=PodcastConfig(topic='Democracy', n_rounds=10, length=10, character_cfg=PodcastCharacters(hosts=[Person(name='Podcast Host', sex=None, description='An NPR Podcast Host who starts and sustains entertaining conversations that aim to inspire meaningful thoughts and perspectives from others.')], guests=[Person(name='Harry Potter', sex=None, description=\"Harry Potter is a fictional character and the titular protagonist in J.K. Rowling's series of fantasy novels. He is a young wizard known for his scar and his fight against the dark wizard Voldemort.\")])) system_prompts={'research': {'solution_coder': \"You are the coder. You write python/shell code to solve the task presented. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor. Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor. If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try. Ensure proper error handling such that an appropriate format of results is returned with the error code.\", 'research_coder': \"You are the Coder. You write python/shell code to gather open-source web information for the task, preferably from Wikipedia or news sources. Provide the code in a code block that is intended to be executed by the executor. The following are the guidelines: Attempt method that only require url requests to work. The user can't modify your code. So do not suggest incomplete code which requires others to modify.  Don't include multiple code blocks in one response.  Do not ask others to copy and paste the result. Check the execution result returned by the executor. If the result indicates there is an error, fix the error and output the code again.  Suggest the full code instead of partial code or code changes.  If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a32132 different approach to try.  Ensure proper error handling such that an appropriate format of results is returned with the error code. \\n\", 'executor': 'Executor. Execute the code written by the Coder and report the result.', 'informer': 'Provide the summarized biograpy of the podcast guests in the conversation to the podcast hosts to prepare for the conversation. The summary must include their most known achievements, personality and relevant news as context that most informs about the character.'}, 'podcast': {'initiation': 'You are going to prepare the host for a podcast among: {characters} in a real-life conversation about {topic} for as long as {length} minutes at 120 words per minute.', 'host': 'As yourself: {0}, respond to the conversation. {parser}', 'guest': 'As yourself: {0}, respond to the conversation. {parser}', 'script_parser': '{parser}. Carefully check the output for correctness, including balanced brackets & encoding.'}, 'epu': {'initiation': 'As a professional clinical psychiatrist. Analyze the following conversation carried out between the characters and their conversation, focusing on the guest participants.  Provide qualitative & quantitaive notes on the conversation to prepare for a comprehensive character traits & perseonality, cognitive and behavioral assessment. The characteres of interests: {characters} The conversation: {conversation}\\n', 'assessment_parser': 'As a professional clinical  psychiatrist, provide a report on each of the characters. {parser}'}}\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "from thought_agents.ontology.config.dialogue import ConversationConfig, PodcastConfig # \n",
    "\n",
    "# Clear Hydra's global state if it is already initialized\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "  GlobalHydra.instance().clear()\n",
    "with initialize(config_path=\"../../conf/dialogue\"):\n",
    "  config = compose(config_name=\"default\")\n",
    "  # Convert the OmegaConf config to the Pydantic model\n",
    "  cfg: ConversationConfig = ConversationConfig(\n",
    "    **OmegaConf.to_container(config, resolve=True)\n",
    "  )\n",
    "\n",
    "# Print the configuration to verify\n",
    "print(cfg)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"conf/OAI_CONFIG_LIST.txt\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-1.5-pro\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"]\n",
    "# [\"gemini-pro-vision\"]\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gemini,\n",
    "    \"timeout\": 120,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from thought_agents.ontology.config.dialogue import *\n",
    "from thought_agents.ontology.parser.dialogue import dialogue_parser, podcast_parser\n",
    "\n",
    "from thought_agents.dialogue.utils import termination_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initializer = autogen.UserProxyAgent(\n",
    "    name=\"init\", \n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"retrieve_coder\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are the Coder. \n",
    "    You write python/shell code to solve the task presented. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "    Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "    If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try. Ensure proper error handling such that an approrpriate format of  results is returned with the error code.\n",
    "    \"\"\",\n",
    ")\n",
    "research_coder = autogen.AssistantAgent(\n",
    "    name=\"research_coder\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"\"\"You are the Coder. You write python/shell code to gather relevant web information for the task. Provide the code in a code block that is intended to be executed by the executor.\n",
    "    The following are the guidelines: \n",
    "    The user can't modify your code. So do not suggest incomplete code which requires others to modify. \n",
    "    Don't include multiple code blocks in one response. \n",
    "    Do not ask others to copy and paste the result. Check the execution result returned by the executor. If the result indicates there is an error, fix the error and output the code again. \n",
    "    Suggest the full code instead of partial code or code changes. \n",
    "    If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try. \n",
    "    Ensure proper error handling such that an appropriate format of results is returned with the error code. \n",
    "    Do not use any method that requires an external API key to work.\n",
    "    \"\"\",\n",
    ")\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"executor\",\n",
    "    system_message=\"Executor. Execute the code written by the Coder and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"outputs/code\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "informer = autogen.AssistantAgent(\n",
    "    name=\"informer\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"\"\"Provide the summarized biograpy of the guests in the conversation to the podcast hosts for starting the conversation. The summary must include their most known achievements, personality and relevant news as context that most informs the character of the guests in the conversation.\"\"\",\n",
    ")\n",
    "\n",
    "script_parser = autogen.AssistantAgent(\n",
    "    name=\"json_parser\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=f\"Ensure all '```json' is converted into a valid JSON. {podcast_parser.get_format_instructions()}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_config = cfg.podcast_config\n",
    "\n",
    "podcast_hosts = [\n",
    "    autogen.ConversableAgent(\n",
    "        name=host.name,\n",
    "        is_termination_msg=termination_msg,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        code_execution_config=False,  # we don't want to execute code in this case.\n",
    "        llm_config=cfg.llm_config.model_dump(),\n",
    "        description=host.description,\n",
    "        system_message=f\"\"\"As yourself: {host.name}, respond to the conversation.\n",
    "        {dialogue_parser.get_format_instructions()}\"\"\",\n",
    "    )\n",
    "    for host in podcast_config.character_cfg.hosts\n",
    "]\n",
    "podcast_gents = [\n",
    "    autogen.ConversableAgent(\n",
    "        name=guest.name,\n",
    "        llm_config=cfg.llm_config.model_dump(),\n",
    "        human_input_mode=\"NEVER\",\n",
    "        system_message=f\"\"\"As yourself: {guest.name}, respond to the conversation. {dialogue_parser.get_format_instructions()}\"\"\",\n",
    "        description=guest.description,\n",
    "    ) for guest in podcast_config.character_cfg.guests\n",
    "]\n",
    "all_agents = [ initializer, research_coder, executor, informer ] + podcast_hosts + podcast_gents + [ script_parser ],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/mymm_psu_gmail_com/hackathon/rag-agents/thought-agents/thought_agents/web/summarizer.py:70: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  map_chain = LLMChain(llm=llm, prompt=self.map_prompt)\n",
      "/home/mymm_psu_gmail_com/hackathon/rag-agents/thought-agents/thought_agents/web/summarizer.py:74: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "source": [
    "from thought_agents.dialogue.agents import agent_registry\n",
    "\n",
    "# podcast chat agents\n",
    "initializer = autogen.UserProxyAgent(\n",
    "    name=\"init\", \n",
    "    code_execution_config=False,\n",
    ")\n",
    "research_agents = agent_registry.get_class(\"dialogue.research\")(\n",
    "    cfg.llm_config, cfg.system_prompts)\n",
    "podcast_hosts, podcast_guests = agent_registry.get_class(\"podcast.characters\")(cfg)\n",
    "script_parser = agent_registry.get_class(\"podcast.parser\")(\n",
    "    cfg.llm_config, cfg.system_prompts)\n",
    "all_agents = [initializer] + research_agents + podcast_hosts + podcast_guests + script_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thought_agents.dialogue.transition import get_state_transition\n",
    "MAX_ROUND=10\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents = all_agents,\n",
    "    messages = [],\n",
    "    max_round=MAX_ROUND ,\n",
    "    speaker_selection_method=get_state_transition(\n",
    "        podcast_config, \"podcast.default\"\n",
    "    ),\n",
    "    # speaker_selection_method=\"round_robin\"\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minit\u001b[0m (to chat_manager):\n",
      "\n",
      "You are going to prepare the host for a podcast among: Harry Potter in a real-life conversation about Democracy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m characters_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(podcast_config\u001b[38;5;241m.\u001b[39mcharacter_cfg\u001b[38;5;241m.\u001b[39mguest_names)\n\u001b[1;32m      2\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDemocracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are going to prepare the host for a podcast among: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcharacters_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m in a real-life conversation about \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtopic\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1018\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1020\u001b[0m     summary_method,\n\u001b[1;32m   1021\u001b[0m     summary_args,\n\u001b[1;32m   1022\u001b[0m     recipient,\n\u001b[1;32m   1023\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:818\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1972\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1972\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   1974\u001b[0m         log_event(\n\u001b[1;32m   1975\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1976\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1980\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   1981\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:1047\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m-> 1047\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   1049\u001b[0m         iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:530\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Select the next speaker (with requery).\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# Prepare the list of available agents and select an agent if selection method allows (non-auto)\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m selected_agent, agents, messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_and_select_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_agent:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_agent\n",
      "File \u001b[0;32m~/miniconda3/envs/ficast/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:407\u001b[0m, in \u001b[0;36mGroupChat._prepare_and_select_agents\u001b[0;34m(self, last_speaker)\u001b[0m\n\u001b[1;32m    405\u001b[0m speaker_selection_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_selection_method\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_selection_method, Callable):\n\u001b[0;32m--> 407\u001b[0m     selected_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_selection_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m selected_agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoEligibleSpeaker(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom speaker selection function returned None. Terminating conversation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hackathon/rag-agents/thought-agents/thought_agents/dialogue/transition.py:26\u001b[0m, in \u001b[0;36mget_state_transition.<locals>.state_transition_wrapper\u001b[0;34m(last_speaker, groupchat, max_round)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstate_transition_wrapper\u001b[39m(\n\u001b[1;32m     24\u001b[0m     last_speaker, groupchat, max_round\u001b[38;5;241m=\u001b[39mMAX_ROUND):\n\u001b[1;32m     25\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstate_transition_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroupchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpodcast_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hackathon/rag-agents/thought-agents/thought_agents/dialogue/transition.py:102\u001b[0m, in \u001b[0;36mfull_podcast_state_transition\u001b[0;34m(last_speaker, groupchat, character_cfg, max_round, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@transition_registry\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpodcast.default\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_podcast_state_transition\u001b[39m(\n\u001b[1;32m    100\u001b[0m     last_speaker, groupchat, character_cfg, max_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    101\u001b[0m     messages \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mmessages\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmax_round\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming the script parser is the last agen\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m last_speaker\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "characters_str = \", \".join(podcast_config.character_cfg.guest_names)\n",
    "topic = \"Democracy\"\n",
    "chat_result = initializer.initiate_chat(\n",
    "    manager, \n",
    "    message=f\"You are going to prepare the host for a podcast among: {characters_str} in a real-life conversation about {topic}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = chat_result.chat_history[-1].get('content').replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "script_json = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_conversation(conv_json, output_dir=Path(\"outputs/conversations\")):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Get the current datetime string\n",
    "    current_datetime_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # Create the output file path\n",
    "    output_file_path = os.path.join(output_dir, f'script_json_{current_datetime_str}.json')\n",
    "    # Write the list of nested JSON objects to the file\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(conv_json, output_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"JSON list saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON list saved to outputs/conversations/script_json_20241015_180437.json\n"
     ]
    }
   ],
   "source": [
    "save_conversation(script_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
